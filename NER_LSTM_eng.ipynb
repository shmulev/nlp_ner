{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING (theano.configdefaults): g++ not available, if using conda: `conda install m2w64-toolchain`\n",
      "C:\\Users\\i344436\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\theano\\configdefaults.py:560: UserWarning: DeprecationWarning: there is no c++ compiler.This is deprecated and with Theano 0.11 a c++ compiler will be mandatory\n",
      "  warnings.warn(\"DeprecationWarning: there is no c++ compiler.\"\n",
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n",
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "#Download your favourite embeddings( e. g. from https://nlp.stanford.edu/projects/glove/ ) and write down path to them\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_path = './data/glove.6B.50d.txt'\n",
    "\n",
    "word2idx = {}\n",
    "word_embeddings = []\n",
    "embedding_size = None\n",
    "#Loading embeddings\n",
    "with open(word_embeddings_path, 'r', encoding=\"utf-8\") as f_em:\n",
    "    for line in f_em:\n",
    "        split = line.strip().split(\" \")\n",
    "        if len(split) <= 2:\n",
    "            continue\n",
    "        if embedding_size is None:\n",
    "            embedding_size = len(split) - 1\n",
    "            # Embeddings iniatilization for paddings and unknown words\n",
    "            word2idx[\"PADDING_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.zeros(embedding_size))\n",
    "\n",
    "            word2idx[\"UNKNOWN_TOKEN\"] = len(word2idx)\n",
    "            word_embeddings.append(np.random.uniform(-0.25, 0.25, embedding_size))\n",
    "        if len(split) - 1 != embedding_size:\n",
    "            continue\n",
    "        word_embeddings.append(np.asarray(split[1:], dtype='float32'))\n",
    "        word2idx[split[0]] = len(word2idx)\n",
    "\n",
    "word_embeddings = np.array(word_embeddings, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idea for improvement: use capitalization templates (as in Nadeau and Sekine 2007)\n",
    "case2idx = {'numeric': 0, 'all_lower':1, 'all_upper':2, 'initial_upper':3, 'other':4, 'mainly_numeric':5, \n",
    "            'contains_digit': 6, 'PADDING_TOKEN':7}\n",
    "case_embeddings = np.identity(len(case2idx), dtype=theano.config.floatX)\n",
    "\n",
    "def get_casing(word, case_lookup):   \n",
    "    casing = 'other'\n",
    "    \n",
    "    num_digits = 0\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            num_digits += 1\n",
    "            \n",
    "    digit_fraction = num_digits / float(len(word))\n",
    "    \n",
    "    if word.isdigit(): #Digit\n",
    "        casing = 'numeric'\n",
    "    elif digit_fraction > 0.5:\n",
    "        casing = 'mainly_numeric'\n",
    "    elif word.islower(): #All lower\n",
    "        casing = 'all_lower'\n",
    "    elif word.isupper(): #All upper\n",
    "        casing = 'all_upper'\n",
    "    elif word[0].isupper(): #First upper,other lower\n",
    "        casing = 'initial_upper'\n",
    "    elif num_digits > 0:\n",
    "        casing = 'contains_digit'  \n",
    "   \n",
    "    return case_lookup[casing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./conll.train 14987 sentences\n",
      "./conll.dev 3466 sentences\n",
      "./conll.test 3684 sentences\n"
     ]
    }
   ],
   "source": [
    "MAX_COLUMNS = 2\n",
    "WORD_COL_NUM = 0\n",
    "LABEL_COL_NUM = 1\n",
    "def read_file(file_path):\n",
    "    \"\"\"\n",
    "    :param file_path: path for corpus in CoNLL-format\n",
    "    :return: corpus_sentences - list of sentences, splitted into words\n",
    "    \"\"\"\n",
    "    corpus_sentences = []\n",
    "    input_sentence = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f_in:\n",
    "        for line in f_in:\n",
    "            line = line.strip()\n",
    "\n",
    "            if len(line) == 0 or line[0] == '#':\n",
    "                if len(input_sentence) > 0:\n",
    "                    corpus_sentences.append(input_sentence)\n",
    "                    input_sentence = []\n",
    "                continue\n",
    "            if len(line.split('\\t')) < MAX_COLUMNS:\n",
    "                print(line)\n",
    "                continue\n",
    "            input_sentence.append(line.split('\\t'))\n",
    "\n",
    "    if len(input_sentence) > 0:\n",
    "        corpus_sentences.append(input_sentence)\n",
    "\n",
    "    print(file_path, len(corpus_sentences), \"sentences\")\n",
    "    return corpus_sentences\n",
    "\n",
    "#Path for parts of CoNLL-2003 corpus\n",
    "train_path = './conll.train'\n",
    "train_sentences = read_file(train_path)\n",
    "\n",
    "dev_path = './conll.dev'\n",
    "dev_sentences = read_file(dev_path)\n",
    "\n",
    "test_path = './conll.test'\n",
    "test_sentences = read_file(test_path)\n",
    "\n",
    "# Often we have one corpus from which dev and test should be splitted (each 0.1-0.2 of corpus). \n",
    "# Impement solution for such situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-MISC': 0, 'I-MISC': 1, 'I-PER': 2, 'B-ORG': 3, 'E-LOC': 4, 'PADDING_LABEL': 5, 'B-PER': 6, 'S-MISC': 7, 'OUT': 8, 'S-PER': 9, 'E-MISC': 10, 'S-LOC': 11, 'E-ORG': 12, 'B-LOC': 13, 'I-LOC': 14, 'I-ORG': 15, 'S-ORG': 16, 'E-PER': 17}\n"
     ]
    }
   ],
   "source": [
    "#Loading all class labels and adding new label for paddings\n",
    "label_set = set()\n",
    "label_set.add('PADDING_LABEL')\n",
    "for dataset in [train_sentences, dev_sentences, test_sentences]:\n",
    "    for sentence in dataset:\n",
    "        for token in sentence:\n",
    "            label = token[LABEL_COL_NUM]\n",
    "            label_set.add(label)    \n",
    "\n",
    "# Turing labels into indices\n",
    "label2idx = {}\n",
    "idx2label = {}\n",
    "for label in label_set:\n",
    "    label2idx[label] = len(label2idx)\n",
    "    \n",
    "print(label2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_indices(token, word2idx, case2idx, unknown_idx):\n",
    "\n",
    "    token_unknown = False\n",
    "    # Each token has several corresponding columns. Token text is in first column\n",
    "    word = token[WORD_COL_NUM]\n",
    "    # First trying to find word in embedding dictionary, if unable trying to find decapitalized word, if unable\n",
    "    # word is considered unknown\n",
    "    if word2idx.get(word) is not None:\n",
    "        word_idx = word2idx[word]\n",
    "    elif word2idx.get(word.lower()) is not None:\n",
    "        word_idx = word2idx[word.lower()]\n",
    "    else:\n",
    "        word_idx = unknown_idx\n",
    "        token_unknown = True\n",
    "\n",
    "    case_idx = get_casing(word, case2idx)\n",
    "    return token_unknown, word_idx, case_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['EU', 'S-ORG'],\n",
       " ['rejects', 'OUT'],\n",
       " ['German', 'S-MISC'],\n",
       " ['call', 'OUT'],\n",
       " ['to', 'OUT'],\n",
       " ['boycott', 'OUT'],\n",
       " ['British', 'S-MISC'],\n",
       " ['lamb', 'OUT'],\n",
       " ['.', 'OUT']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204567 tokens, 5578 unknown, 2.73%\n",
      "51578 tokens, 1270 unknown, 2.46%\n",
      "46666 tokens, 1562 unknown, 3.35%\n",
      "[array([0, 1, 0]), array([7, 2, 7]), array([5, 8, 5])]\n",
      "[array([    0,   646,  7580,   516,   582,     6,  5262,   299, 10240,\n",
      "           4,     0]), array([7, 2, 1, 3, 1, 1, 1, 3, 1, 4, 7]), array([ 5, 16,  8,  7,  8,  8,  8,  7,  8,  8,  5])]\n",
      "[array([   0, 1296, 9005,    0]), array([7, 3, 3, 7]), array([ 5,  6, 17,  5])]\n",
      "[array([   0, 3881,    1,    0]), array([7, 2, 5, 7]), array([ 5, 11,  8,  5])]\n",
      "[array([    0,     2,   293,   629,    18,    15,   186,    22, 10468,\n",
      "          19,   516,  3242,     6,  2036,     6, 18291,   299, 10240,\n",
      "         209,  2156,  2389,   403,  5123,  6474,  1291,    88,    32,\n",
      "        7595,     6,  7377,     4,     0]), array([7, 3, 3, 3, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1,\n",
      "       1, 1, 1, 1, 1, 1, 1, 1, 4, 7]), array([ 5,  8,  3, 12,  8,  8,  8,  8,  8,  8,  7,  8,  8,  8,  8,  8,  7,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  8,  5])]\n"
     ]
    }
   ],
   "source": [
    "def create_matrices(sentences, word2idx, label2idx, case2idx):   \n",
    "    \n",
    "    unknown_idx = word2idx['UNKNOWN_TOKEN']\n",
    "    padding_casing = case2idx['PADDING_TOKEN']\n",
    "    padding_idx = word2idx['PADDING_TOKEN'] \n",
    "    padding_label = label2idx['PADDING_LABEL']  \n",
    "    \n",
    "    dataset = []\n",
    "    total_tokens = 0\n",
    "    unknown_tokens = 0\n",
    "    for sentence in sentences:\n",
    "        \n",
    "        # Index of first non-padding in sentence\n",
    "        proper_sentence_start = 1\n",
    "\n",
    "        word_indices = np.array([padding_idx] * (len(sentence) + 2))\n",
    "        case_indices = np.array([padding_casing] * (len(sentence) + 2))\n",
    "        label_indices = np.array([padding_label] * (len(sentence) + 2))\n",
    "        \n",
    "        #Here each sentence starts with and ends with exactly one padding\n",
    "        #However when using GPUs each sentence in one batch should have identical shapes. Thus sentences should be splitted\n",
    "        #into such groups that each sentence in one group should have identical lengths (and thus sentences of one group can\n",
    "        #be present in one batch). The easiest way to do this is to have all sentences padded up to max length(i. e. have \n",
    "        #1 group). Most sentnces are short but in most corpora there exist really long ones thus this solution has massive \n",
    "        #overhead. Thus it is more reasonable to have more sophisticated grouping principle e. g. padding sentence to \n",
    "        #length of nearest 2^n + 1\n",
    "\n",
    "        for pos_in_sentence, word in enumerate(sentence):\n",
    "\n",
    "            token_unknown, word_idx, case_idx = get_token_indices(word, word2idx, case2idx, unknown_idx)\n",
    "            pos_in_padded_sentence = pos_in_sentence + proper_sentence_start\n",
    "            word_indices[pos_in_padded_sentence] = word_idx\n",
    "            case_indices[pos_in_padded_sentence] = case_idx\n",
    "            label_indices[pos_in_padded_sentence] = label2idx[word[LABEL_COL_NUM]]\n",
    "\n",
    "            # Calculating percent of tokens not covered by embeddings\n",
    "            total_tokens += 1\n",
    "            if token_unknown:\n",
    "                unknown_tokens += 1\n",
    "\n",
    "        # All data for one sentence put in one list\n",
    "        dataset.append([word_indices, case_indices, label_indices])\n",
    "        \n",
    "    percent = 0.0\n",
    "    if total_tokens != 0:\n",
    "        percent = float(unknown_tokens) / total_tokens * 100\n",
    "    print(\"{} tokens, {} unknown, {:.3}%\".format(total_tokens, unknown_tokens, percent ))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "\n",
    "train_data = create_matrices(train_sentences, word2idx, label2idx, case2idx)\n",
    "dev_data = create_matrices(dev_sentences, word2idx, label2idx, case2idx)\n",
    "test_data = create_matrices(test_sentences, word2idx, label2idx, case2idx)\n",
    "\n",
    "for sentence in train_data[:5]:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "tokens_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "casing_input (InputLayer)       (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "tokens_embeddings (Embedding)   (None, None, 50)     20000100    tokens_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "casing_embeddings (Embedding)   (None, None, 8)      64          casing_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "merged_embeddings (Concatenate) (None, None, 58)     0           tokens_embeddings[0][0]          \n",
      "                                                                 casing_embeddings[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, None, 58)     0           merged_embeddings[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "blstm (Bidirectional)           (None, None, 20)     5520        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, None, 18)     378         blstm[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 20,006,062\n",
      "Trainable params: 5,962\n",
      "Non-trainable params: 20,000,100\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Idea for improvement: implement char features by taking first k or last k symbols (or both) for each token and\n",
    "#map through convolutional or recurrent layer\n",
    "# Resulting vector should be merged with other token features into merged_embeddings\n",
    "# More detailed description https://arxiv.org/pdf/1603.01360v1.pdf \n",
    "\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, TimeDistributed, Dropout, Bidirectional, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "# Practical implementations should have dim of 100 or more\n",
    "SENTENCE_LSTM_DIM = 10\n",
    "\n",
    "n_out = len(label2idx)\n",
    "\n",
    "tokens_input = Input(dtype='int32', shape=(None,), name='tokens_input')\n",
    "tokens_embedding_layer = Embedding(input_dim=word_embeddings.shape[0], \n",
    "                                   output_dim=word_embeddings.shape[1],\n",
    "                                   weights=[word_embeddings], trainable=False, \n",
    "                                   name='tokens_embeddings')\n",
    "tokens = tokens_embedding_layer(tokens_input)\n",
    "\n",
    "\n",
    "casing_input = Input(dtype='int32', shape=(None,), name='casing_input')\n",
    "casing_embedding_layer = Embedding(input_dim=case_embeddings.shape[0], \n",
    "                                   output_dim=case_embeddings.shape[1],\n",
    "                                   weights=[case_embeddings], trainable=True, \n",
    "                                   name='casing_embeddings')\n",
    "casing = casing_embedding_layer(casing_input)\n",
    "\n",
    "merged_embeddings = concatenate([tokens, casing], name='merged_embeddings')\n",
    "for_lstm = Dropout(0.2)(merged_embeddings)\n",
    "# If GPU is used  choose implementation=2\n",
    "blstm = Bidirectional(LSTM(SENTENCE_LSTM_DIM, return_sequences=True, implementation=1), \n",
    "                      name='blstm')(for_lstm)\n",
    "#Try several architectures here - GRU, convolutions, several recurrent layers stacked etc \n",
    "#result = Conv1d(n_out,1, activation='softmax', name='result'))(blstm\n",
    "result = TimeDistributed(Dense(n_out, activation='softmax', name='result'))(blstm)\n",
    "\n",
    "model = Model(inputs=[tokens_input, casing_input], outputs=result)\n",
    "\n",
    "# default lr = 0.001, beta_1=0.9\n",
    "adam = Adam(lr=0.001, beta_1=0.9)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=adam)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 epochs\n",
      "14987 train sentences\n",
      "3466 dev sentences\n",
      "3684 test sentences\n",
      "--------- Epoch 0 -----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-051332ee418a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;31m#print(batch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasing\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%.2f sec for training\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1881\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1882\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1883\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1884\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1885\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[1;32m-> 2482\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2483\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    898\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 900\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    901\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1133\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1135\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1136\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1316\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1317\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1320\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1307\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1309\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1409\u001b[1;33m           run_metadata)\n\u001b[0m\u001b[0;32m   1410\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1411\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "# If sentences were successfully groupt into groups of identical length here you should yield list of BATCH_SIZE \n",
    "# sentences of one length (BATCH_SIZE is such that one batch can be loaded into GPU memory)\n",
    "def iterate_minibatches(dataset):   \n",
    "    for sentence in dataset:\n",
    "        tokens, casing, labels = sentence     \n",
    "            \n",
    "        labels = np.expand_dims(labels, -1) \n",
    "        yield np.asarray([tokens]), np.asarray([casing]), np.asarray([labels])\n",
    "\n",
    "# Here again code should be adapted for batches of sentences     \n",
    "def tag_dataset(dataset):\n",
    "    predicted_labels = []\n",
    "    correct_labels = []\n",
    "    for tokens, casing, labels in dataset:\n",
    "        pred = model.predict_on_batch([np.asarray([tokens]), np.asarray([casing])])[0]\n",
    "        pred_labels = [el.tolist().index(max(el)) for el in pred]\n",
    "        predicted_labels.append(pred_labels)\n",
    "        correct_labels.append(labels)\n",
    "        #print(predicted_labels, correct_labels)\n",
    "    return predicted_labels, correct_labels\n",
    "\n",
    "# The best evaluation metric is f-measure on entities constucted from several tokens.  \n",
    "# See https://github.com/mit-nlp/MITIE/blob/master/tools/ner_conll/conlleval for reference\n",
    "def compute_accuracy(predictions, correct, padding_label):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    total_tokens = 0\n",
    "    guessed_tokens = 0\n",
    "    for guessed_sentence, correct_sentence in zip(predictions, correct):\n",
    "        #print(guessed_sentence, correct_sentence)\n",
    "        assert (len(guessed_sentence) == len(correct_sentence)), \"Guessed and correct sentences do not match\"\n",
    "        for j in range(len(guessed_sentence)):\n",
    "            if correct_sentence[j] != padding_label:\n",
    "                total_tokens += 1\n",
    "                if guessed_sentence[j] == correct_sentence[j]:\n",
    "                    guessed_tokens += 1\n",
    "\n",
    "    if total_tokens == 0:\n",
    "        return float(0)\n",
    "    else:\n",
    "        accuracy = float(guessed_tokens) / total_tokens\n",
    "        return accuracy\n",
    "\n",
    "        \n",
    "number_of_epochs = 10\n",
    "print(\"%d epochs\" % number_of_epochs)\n",
    "\n",
    "print(\"%d train sentences\" % len(train_data))\n",
    "print(\"%d dev sentences\" % len(dev_data))\n",
    "print(\"%d test sentences\" % len(test_data))\n",
    "\n",
    "padding_label = label2idx['PADDING_LABEL']\n",
    "\n",
    "for epoch in range(number_of_epochs):    \n",
    "    print(\"--------- Epoch %d -----------\" % epoch)\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    start_time = time.time()    \n",
    "    for batch in iterate_minibatches(train_data):\n",
    "        #print(batch)\n",
    "        tokens, casing, labels = batch       \n",
    "        model.train_on_batch([tokens, casing], labels)   \n",
    "    print(\"%.2f sec for training\" % (time.time() - start_time))\n",
    "               \n",
    "    #Train Dataset       \n",
    "    start_time = time.time()  \n",
    "    print(\"================================== Train Data ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(train_data)        \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "    #Dev Dataset \n",
    "    print(\"================================== Dev Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(dev_data)  \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "\n",
    "    #Test Dataset \n",
    "    #state-of-the-art f-мера~0.91 на test\n",
    "    print(\"================================== Test Data: ==================================\")\n",
    "    predicted_labels, correct_labels = tag_dataset(test_data)  \n",
    "    accuracy = compute_accuracy(predicted_labels, correct_labels, padding_label)\n",
    "    print(\"Accuracy = \", accuracy)\n",
    "\n",
    "        \n",
    "    print(\"%.2f sec for evaluation\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Improvements for credit\n",
    "# 1. Create train-test-dev split function form one corpus\n",
    "# 2. Instead of 8 casing types use general capitalization templates\n",
    "# 3. Orginize sentence padding to (2^n + 1) tokens where (2^n + 1) is closest to current sentence length\n",
    "# 4. Given item 3 realization organize reasanable work with batches i. e. make iterate_minibatches yield batches of\n",
    "#  BATCH_SIZE sentences of same padded length(necessary if you want to utilize learning on GPU properly)\n",
    "# 5. Add symbol features to neural net- either feed embeddings of CHAR_SIZE first and CHAR_SIZE last symbols of each token\n",
    "# to LSTM(or GRU) or feed embeddings of all symbols of token to CNN. The resulting vector of this layer(it should be\n",
    "# of same length for every token) has to be concatenated with word embeddings and other token features\n",
    "# 6. Conduct experiments with architecture: try stacking several LSTM instead of 1, try replacing it with GRU or CNN etc.\n",
    "# 7. Instead of accuracy evaluate with f-measure on entities constucted from several tokens as in conlleval\n",
    "# If all the improvements are implemented correcly you should be able to achieve results close to state-of-the-art:\n",
    "# f-measure on test-set >= 0.905"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
